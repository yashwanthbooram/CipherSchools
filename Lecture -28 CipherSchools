Feature Engineering & Selection 

Feature Engineering Methods

1. Handling Missing Values
• Imputation:
    • Fill missing values with mean, median, mode, or other values.

Example:
Feature 1                      Feature 2                       Feature 3
0.1                            0.2                             NaN
0.2                            NaN                             0.6
NaN                            0.6                             0.7

After imputation:
Feature 1                      Feature 2                       Feature 3
0.1                            0.2                             0.65
0.2                            0.4                             0.6
0.15                           0.6                             0.7


2. Encoding Categorical Variables
• One-Hot Encoding:
    • Convert categorical variables into a series of binary columns.
Example:
Color:
• Red
• Blue
• Green

After one-hot encoding:
Color_Red                      Color_Blue                      Color_Green
1                              0                               0
0                              1                               0
0                              0                               1


3. Feature Scaling
• Min-Max Scaling:
    • Scale features to a fixed range, typically [0, 1].
Example:
Feature 1                      Feature 2
10                             100
20                             200
30                             300

After min-max scaling:
Feature 1                      Feature 2
0                              0
0.5                            0.5
1                              1


4. Feature Creation
• Polynomial Features:
    • Create new features by taking polynomial combinations of existing features.
Example:
Feature 1                      Feature 2
1                              2
3                              4
5                              6

After creating polynomial features (degree=2):
Feature 1          Feature 2          Feature 1^2        Feature 2^2          Feature 1*Feature 2        
1                  2                  1                  4                    2
3                  4                  9                  16                   12
5                  6                  25                 36                   30

FEATURE SELECTION METHODS ``````````````````````````
1. Managing Value Missing
Definition: In data preprocessing, handling missing values is essential. 
Imputation (replacing missing values with the mode, mea medies, or a specified value) and row or column removal with missing values are examples of initial techniques.
import pandas as pd 
from sklearn.impute import SimpleImputer
#Sample data
data = {
'Featurel': [1.0, 2.0, None, 4.0, 5.0], 'Feature2': [2.0, None, 4.0, 5.0, None), 'Feature3': [None, 3.0, 3.5, 4.0, 4.5)
}
df = pd.DataFrame(data)
#Handling missing values
imputer = SimpleImputer (strategy='mean")
df_imputed = pd.DataFrame(imputer.fit_transform(df), columns-df.columns)
print("After Imputation:\n", df_imputed)

2. Definition of Encoding for Categorical Variables:

The process of transforming categorical data into a numerical representation that machine learning algorithms can use is known as encoding categorical variables.
Label encoding and one-hot encoding are common techniques.
Example Table data:
Color
• Red
• Blue
• Green
• Blue
• Red

import pandas as pd
from sklearn.preprocessing import OneHotEncoder
#Sample data
data = {
"Color': ['Red', 'Blue', 'Green', 'Blue', 'Red']\
}
df = pd.DataFrame(data)
#Encoding categorical variables
encoder = OneHotEncoder(sparse=False)
encoded_categories = encoder.fit_transform(df[['Color']])
df_encoded = pd.DataFrame(encoded_categories, columns encoder.get_feature_names_out(['Color']))
df = pd.concat([df, df_encoded), axis=1).drop("Color', axis=1)
print("After One-Hot Encoding:\n", df)

3. Feature Scaling Definition:

Normalizing or standardizing features to give them a comparable scale is known as feature scaling. 
Min-max scaling and standardization (z-score normalization) are two popular techniques.

Example Table Data:
Feature 1                      Feature 2
10                             100
20                             200
30                             300
40                             400
50                             500

import pandas as pd 
from sklearn.preprocessing import MinMaxScaler
#Sample data
data = {
'Featurel': [10, 20, 30, 40, 50],
'Feature2': [100, 200, 300, 400, 500]
}
df = pd.DataFrame(data)
#Feature scaling  
scaler = MinMaxScaler()
df_scaled = pd.DataFrame (scaler.fit_transform(df), columns-df.columns) 
print("After Min-Max Scaling:\n", df_scaled)

4. Definition of Feature Creation:

In order to increase the prediction capacity of machine learning models, feature generation entails creating new features from preexisting ones.
Polynomial features and interaction terms are common techniques.
import pandas as pd
from sklearn.preprocessing import Polynomial Features
#Sample data
data = {
'Feature1': [1, 2, 3, 4, 5], 'Feature2': [2, 3, 4, 5, 6]
}
df = pd.DataFrame(data)
# Feature creation
poly = PolynomialFeatures(degree-2, include_bias=False)
poly_features = poly.fit_transform(df)
df polypd.DataFrame(poly_features, columns-poly.get_feature_names_out(['Feature1", "Feature2"])) 
print("After Creating Polynomial Features:\n", df poly)

FEATURE SELECTION METHODS

1. Definition of Variance Thresholding:
A basic method for selecting features is variance thresholding. 
Any traits whose variance is less than a certain threshold are eliminated. 
All zero-variance characteristics—that is, features with the same value across samples—are by default eliminated.

Example Table Data
Feature 1        Feature2        Feature3        Constant
1                2               3               1
1                3               4               1
1                4               5               1
1                5               6               1
1                6               7               1
In this table, 'Feature 1' and 'Constant' have low or zero variance.

2. The Meaning of Correlation Matrix Filtering

Matrix of Correlations The process of filtering entails calculating the dataset's correlation matrix and eliminating one feature from each pair of highly correlated characteristics.
This contributes to lessening data redundancy.
Example Table Data
Feature 1        Feature2        Feature3        Feature4
1                2               2               5
2                4               4               6
3                6               6               7
4                8               8               8
5                10              10              9
In this table, 'Feature2' and 'Feature3' are highly correlated with 'Feature1'.

3. Domain Knowledge Explanation:

utilizing domain knowledge entails hand-picking the most pertinent features utilizing understanding of the particular profession or industry. 
This approach makes use of human perception of the traits that are most likely to be significant.

Example Table Data
Age        Salary        Height        Weight
25         50000         5.5           150
30         60000         6.0           160
35         70000         5.8           170
40         80000         5.9           180
45         90000         6.1           190
In this table, 'Age' and 'Salary' might be selected based on domain knowledge indicating their importance.


