Supervised Learning (SVMs, Random Forests, Decision Trees, Logistic Regression, and Linear and Logistic Regression)

Overview of Supervised Learning: Supervised learning is a core machine learning technique in which new, unseen data is used to train an algorithm based on labeled data. 
This robust method has several uses, ranging from medical diagnosis to spam screening.

• Overfitting (High variance): This occurs when a model overfits to particular patterns found in the training dataset and underperforms when applied to new data. 
Excessive complexity in models can lead to overfitting.
- Causes of the Overfitting:
  1. An overly intricate model.
  2. Excessive training.
  3. Insufficient training set.


• Underfitting (High bias): When the algorithm is unable to capture pertinent relationships between features and desired outputs, underfitting (high bias) happens. 
Generally speaking, a high bias model makes more assumptions about the goal function or outcome.
- Causes of the Underfit:
  1. The model is overly simplistic.
  2. Not enough instruction.
  3. A poor choice of features.

• Good balance (optimal/low bias/low variance): 
A model with low variance and low bias can be said to be sensitive enough to not be overly affected by changes in the training set of data, while also being able to identify underlying patterns in the data.

ALGORITHMS ``````````

1. Linear Regression: A Comprehensive Overview of Continuous Relationships
  A useful method for comprehending the continuous relationships between variables is linear regression. 
With the use of linear regression, you may create predictive models that project future results by finding patterns in data.
Determining the factors that characterize the direction and intensity of the correlations between your input and output variables is crucial.

- Significance: • Recognize Trends: Discover the relationships between variables.
• Predicting Outcomes: Using incoming data, project future values.
• Estimating Parameters: Establish the direction and strength of the connections.

from sklearn.model selection import train test split 
from sklearn.linear model import LinearRegression 
from sklearn.metrics import mean_squared_error
#Generating synthetic data 
import numpy as np 
X = np.random.rand(100, 1) * 10
y = 2.5 * X + np.random.randn(100, 1) * 2
#Splitting data into training and testing sets 
X_train, X_test, y train, y test train test split(X, y, test sizes=0.2, random state=42)
#Training the model 
model = LinearRegression() 
model.fit(X_train, y_train)
# Making predictions 
y_pred = model.predict(X_test)
#Evaluating the model 
mse = mean_squared_error(y_test, y_pred) 
print('Mean Squared Error: (mse)')

2- Logistic Regression: Predicting Binary Outcomes:
                            
                           Input Data
                (Categorical or numerical features)
                                ⬇
                          Transformation
          (Apply the logistic funciton to predict possibility)
                                ⬇
                          Classification
          (Determine class based on probability threshold)
• z = bo + b₁x₁ + ... + brXr
• p(x) = 1 / 1+ e^-z
• Sigmoid function : σ(z) = 1 / 1+ e^-z

fron sklearn.datasets import load_iris 
from sklearn.model_selection import train_test_split 
from sklearn.linear_model import LogisticRegression 
from sklearn.metrics import accuracy_score
#Loading the Iris dataset
iris = load_iris() 
X = iris.data 
y = iris.target
#Using only two classes for binary classification
X = X[y != 2] 
y = y[y != 2]
#Splitting data into training and testing sets 
X_train, X_test, y_train, y_test = train_test_split(X, y, test size=0.2, random state=42)
#Training the model
model = LogisticRegression()
model.fit(X_train, y_train)
#Making predictions
y_pred = model.predict(X_test)
#Evaluating the model \
accuracy = accuracy_score(y_test, y_pred) 
print(f'Accuracy: {accuracy}')

3-Decision Trees: Constructing a Hierarchical Framework
• Choosing Features:
  Determine the key elements that will influence the decision-making process. In order to build a successful decision tree, this is an essential first step.
• Partitioning Recursively
  By continually dividing the data according to the features, the decision tree algorithm produces a hierarchical tree-like structure of choices and results.

  ROOT Node
                                                 /\
                                                /  \
                                               /    \
                                  Decision Node      Desicion Node
                                   /\                           /\
                                  /  \                         /  \
                                 /    \           Terminal Node    Terminal Node
                    Terminal Node      Decision Node
                                       /\
                                      /  \
                                     /    \
                        Terminal Node      Terminal Node
from sklearn.datasets import load_iris 
from sklearn.model_selection import train_test_split 
from sklearn.tree import DecisionTreeClassifier 
from sklearn.metrics import accuracy_score
#Loading the Iris dataset 
iris = load iris() 
X= iris.data 
y = iris.target
#Splitting data into training and testing sets
X_train, X_test, y train, y_test = train_test_split(X, y, test size=0.2, random_state=42)
#Training the model 
model = DecisionTreeClassifier() 
model.fit(X_train, y_train)
#Making predictions
y_pred = model.predict(X_test)
#Evaluating the model \
accuracy = accuracy_score(y_test, y_pred) 
print(f'Accuracy: {accuracy}')

Support Vector Machines (SVMs):
1 - Maximize Margin: SVM identifies the optimal hyperplane that maximizes the distance between data points of different classes.
2 - Kernel Trick: By using kernel functions, SVMs can efficiently handle non-linear problems in high dimensional spaces.
3 - Robust to Outliers: SVMs are less sensitive to outliers compared to other classification algorithms.

from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split 
from sklearn.svm import SVC 
from sklearn.metrics import accuracy_score
# Loading the Iris dataset
iris load_iris()
X = iris.data 
y = iris.target
# Using only two classes for binary classification
X = X[y != 2]
y = y[y != 2]
#Splitting data into training and testing sets
X_train, X_test, y_train, y test train_test = split(x, y, test size=0.2, random state=42)
# Training the model 
model = SVC() 
model.fit(X_train, y_train)
# Making predictions 
y_pred = model.predict(X_test)
# Evaluating the model 
accuracy = accuracy_score(y_test, y_pred) 
print(f'Accuracy: {accuracy}')

